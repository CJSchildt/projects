{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Forecasting for univariate time series\n",
    "# https://datascience.stackexchange.com/questions/12721/time-series-prediction-using-arima-vs-lstm\n",
    "# above link was a useful discussion on the state of producing prediction/confidence\n",
    "#The following notebook was adapted from \n",
    "#https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
    "#to apply a \"walk forward\" forecasting model where a point is predicted and validated \n",
    "#then the model refit with the actual point appended (as if we are predicting only the next point at every step)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import temp dataset\n",
    "\n",
    "fdir = \"C:/Users/...\"\n",
    "file  = \"time_series.xlsx\"\n",
    "\n",
    "dataraw = pd.read_excel(fdir + '/' + file, sheet_name=\"data\", usecols=[4])\n",
    "dataclean = dataraw[:-5].copy().dropna()\n",
    "\n",
    "outliers_fraction = .01 #fraction of normal distribution that accounts for 3, 4, 5 stds is 0.003, 0.00006, 0.0000006, respectively\n",
    "n_outliers = int(outliers_fraction*dataclean.describe().loc[\"count\",\"Temp\"])\n",
    "# create list of indexes to modify\n",
    "index_outliers = pd.DataFrame(random.sample(range(0,int(dataclean.describe().loc[\"count\",\"Temp\"]+1)), n_outliers) )\n",
    "\n",
    "dataclean_mean = float(dataclean.describe().loc[\"mean\",\"Temp\"])\n",
    "dataclean_std = float(dataclean.describe().loc[\"std\",\"Temp\"])\n",
    "#print(dataclean_mean, dataclean_std)\n",
    "\n",
    "random.seed(18)\n",
    "datadirty = dataclean.copy()\n",
    "datadirty_class = np.zeros(len(dataclean))\n",
    "datadirty_class = pd.DataFrame(datadirty_class, columns=['class'])\n",
    "\n",
    "for i, row in index_outliers.iterrows():\n",
    "\tpoint_temp = float(dataclean.loc[row,\"Temp\"])\n",
    "\t# if point_temp >= dataclean_mean:\n",
    "\tdirty_temp = point_temp + ((random.random() - 0.5)*2*dataclean_std*5)\n",
    "\t# if point_temp < dataclean_mean:\n",
    "\t# \tdirty_temp = point_temp - (random.random()*dataclean_std*4)\n",
    "\tdatadirty.at[row,\"Temp\"] = dirty_temp\n",
    "\tdatadirty_class.at[row,\"class\"] = 1\n",
    "\n",
    "datadirty_class = datadirty_class.as_matrix()\n",
    "dataclean = dataclean[\"Temp\"].as_matrix()\n",
    "datadirty= datadirty[\"Temp\"].as_matrix().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data is broken into training and testing sets\n",
    "# train_step and dataclean_dwn_sample are included as default to provide faster computation\n",
    "\n",
    "train_step = int(60) # sample equivalent of 4 minutes\n",
    "dataclean_dwn_samp = dataclean[::train_step].copy()\n",
    "\n",
    "two_days = int(43200/train_step) #in samples\n",
    "#two_days = int(43200) #raw time scale\n",
    "train_size = two_days #in hours\n",
    "n_forecast_pts = len(datadirty[::train_step]) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate a sequence class using timeseriesgenerator on datadirty\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "#def parser(x):\n",
    "#\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# frame a sequence as a supervised learning problem by shifting so that t+1 is the \"solution\" of t\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = pd.concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn pd.Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = numpy.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform data to be stationary and prepare for training/predicting\n",
    "#raw_values = series.values\n",
    "raw_values = dataclean_dwn_samp\n",
    "diff_values = difference(raw_values, 1)\n",
    "\n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    "\n",
    "# split data into train and test-sets\n",
    "# train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    "train, test = supervised_values[0:train_size], supervised_values[train_size:]\n",
    "\n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model\n",
    "lstm_model = fit_lstm(train_scaled, 1, 1000, 8)\n",
    "# forecast the entire training dataset to build up state for forecasting\n",
    "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "lstm_model.predict(train_reshaped, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "for i in range(len(test_scaled)):\n",
    "\t# make one-step forecast\n",
    "\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t# invert scaling\n",
    "\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t# invert differencing\n",
    "\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t# store forecast\n",
    "\tpredictions.append(yhat)\n",
    "\texpected = raw_values[len(train) + i + 1]\n",
    "\tprint('Hour=%d, Predicted=%f, Expected=%f' % (i+1+48, yhat, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## report performance\n",
    "rmse = sqrt(mean_squared_error(raw_values[two_days:two_days+len(predictions)], predictions))\n",
    "print(rmse.describe())\n",
    "\n",
    "## Wilson Score Interval for computing 95% CI bounds\n",
    "#https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for generating error bounds\n",
    "\n",
    "error = rmse.copy()\n",
    "#!!! CI cannot be computed with errors larger than 1,\n",
    "# the below method is more appropriate for classification error, however\n",
    "# a transform has been applied to reframe RMSE in terms of perecnt of maximum RMSE observed\n",
    "\n",
    "##\n",
    "scaler_error = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_error = scaler_error.fit(error.reshape(-1,1))\n",
    "scaled_error = scaler_error.transform(error.reshape(-1,1))\n",
    "# print(scaled_error)\n",
    "n = len(error)\n",
    "const = 1.96 # constant associated with 95% CI interval\n",
    "CIu = scaled_error + const*np.sqrt((scaled_error * (1 - scaled_error)) / n)\n",
    "CIl = scaled_error - const*np.sqrt((scaled_error * (1 - scaled_error)) / n)\n",
    "##\n",
    "#!!! the following may work to convert errors back into RMSE scale, however\n",
    "# I don't believe this is a proper inversion for the desired outcome\n",
    "CIu_rmse = scaler_error.inverse_transform(CIu)\n",
    "CIl_rmse = scaler_error.inverse_transform(CIl)\n",
    "\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot of observed vs predicted\n",
    "pyplot.plot(raw_values[two_days:])\n",
    "pyplot.plot(predictions)\n",
    "pyplot.plot(predictions+CIu_rmse, linestyle='--')\n",
    "pyplot.plot(predictions-CIl_rmse, linestyle='--')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
